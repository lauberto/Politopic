{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lauberto/politopic/blob/main/notebook/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G88tTx3LYG3H"
   },
   "source": [
    "# Topic Modelling on Politicians' speech\n",
    "\n",
    "This notebook shows how to apply topic modelling on the transcription of yt videos with Politicians' speech. The transcription is performed with [`whisper`](https://github.com/openai/whisper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33vwa3iihjsH"
   },
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKiJ9X7Ao6TD",
    "outputId": "7fe871ae-2290-4616-ad6c-fbb41820b9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaXoSMK-esAf",
    "outputId": "0caef8fe-fdd1-461f-b1d1-0e737bf68999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [1,073 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,099 kB]\n",
      "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,352 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,311 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,567 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,524 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,342 kB]\n",
      "Fetched 14.5 MB in 4s (3,249 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "34 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'sudo apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update && sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OjGFAlTZxHq",
    "outputId": "c8dd6af2-a963-4dec-ff0c-b11e585f801d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-f8ws7i94\n",
      "  Running command git clone -q https://github.com/openai/whisper.git /tmp/pip-req-build-f8ws7i94\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (1.21.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (1.13.0+cu116)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (4.64.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from whisper==1.0) (9.0.0)\n",
      "Collecting transformers>=4.19.0\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 33.6 MB/s \n",
      "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.16.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 64.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (3.8.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 76.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0->whisper==1.0) (2022.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.19.0->whisper==1.0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (3.0.4)\n",
      "Building wheels for collected packages: whisper\n",
      "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175324 sha256=23abc57c4f30c8a2c9f59d770da924256961c1245428e4bf215012a606204697\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-x4y2_8lm/wheels/a7/70/18/b7693c07b1d18b3dafb328f5d0496aa0d41a9c09ef332fd8e6\n",
      "Successfully built whisper\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, ffmpeg-python, whisper\n",
      "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1 whisper-1.0\n"
     ]
    }
   ],
   "source": [
    "# Installing whisper\n",
    "!pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEAJ18xNlzBl",
    "outputId": "7bece403-bc86-481d-8a1b-4d8731635ff4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct4adIANpQvX"
   },
   "source": [
    "\n",
    " Installing [`pytube`](https://https://github.com/pytube/pytube). and downloading the [video](https://https://www.youtube.com/watch?v=knj8ULToNvo) from youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oavpZpUXkLp",
    "outputId": "ebb26ec1-ba4a-458e-ff1a-9727c9b880e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://__token__:****@gitlab.welocalize.com/api/v4/projects/327/packages/pypi/simple\n",
      "Collecting git+https://github.com/pytube/pytube\n",
      "  Cloning https://github.com/pytube/pytube to /private/var/folders/my/7gpsbyln179fyxzztd61gwwc0000gp/T/pip-req-build-jezhenj3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pytube/pytube /private/var/folders/my/7gpsbyln179fyxzztd61gwwc0000gp/T/pip-req-build-jezhenj3\n",
      "  Resolved https://github.com/pytube/pytube to commit 84faec34c8a66f502ac635a5610445dbff160654\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/pytube/pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C_j_VCN_oXP4"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Qq3vJeBBA-_"
   },
   "outputs": [],
   "source": [
    "def parse_response(search_response: dict):\n",
    "\tvideos = []\n",
    "\tchannels = []\n",
    "\tplaylists = []\n",
    "\t\n",
    "\tfor search_result in search_response.get('items', []):\n",
    "\t\tif search_result['id']['kind'] == 'youtube#video':\n",
    "\t\t\tvideos.append({\n",
    "\t\t\t\t'title': search_result['snippet']['title'],\n",
    "\t\t\t\t'video_id':search_result['id']['videoId']\n",
    "\t\t\t})\n",
    "\t\telif search_result['id']['kind'] == 'youtube#channel':\n",
    "\t\t\tchannels.append({\n",
    "\t\t\t\t'title': search_result['snippet']['title'],\n",
    "\t\t\t\t'channel_id': search_result['id']['channelId']\n",
    "\t\t\t})\n",
    "\t\telif search_result['id']['kind'] == 'youtube#playlist':\n",
    "\t\t\tplaylists.append({\n",
    "\t\t\t\t'title': search_result['snippet']['title'],\n",
    "\t\t\t\t'playlist_id': search_result['id']['playlistId']\n",
    "\t\t\t})\n",
    "\n",
    "\treturn {\n",
    "\t\t'videos': videos,\n",
    "\t\t'channels': channels,\n",
    "\t\t'playlists': playlists\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMh3_NUW_1IB",
    "outputId": "6ca944dc-4084-4eba-a030-e9595eb8c701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API key: ········\n"
     ]
    }
   ],
   "source": [
    "api_key = getpass(\"Google API key: \")\n",
    "channel_id = \"UCDjM54fZ-cD7F8uom767OhA\" # Salvini Channel\n",
    "# channel_id = \"UCw2AF_J2QizzmWw99bs3e6Q\" # Lega Channel\n",
    "max_results = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kdKNL67T_hlh"
   },
   "outputs": [],
   "source": [
    "res = requests.get(f\"https://www.googleapis.com/youtube/v3/search?key={api_key}&channelId={channel_id}&part=snippet,id&order=date&maxResults={max_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xbeYxuJE_oZW"
   },
   "outputs": [],
   "source": [
    "res_dict = dict(json.loads(res.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dX6fOvxPBC6n"
   },
   "outputs": [],
   "source": [
    "parsed_res = parse_response(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puQtPruAppoa",
    "outputId": "068e7393-564f-44e7-92fb-f07e5f68788a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'QUESTION TIME MINISTRO SALVINI (SENATO, 01.12.2022)',\n",
       "  'video_id': 'pwhRDzz1UWk'},\n",
       " {'title': 'HOW CAN WE GOVERN EUROPE? 9ª EDIZIONE (ROMA, 30.11.2022)',\n",
       "  'video_id': 'KYR_IqIP2YI'},\n",
       " {'title': 'LOMBARDIA 2030: PROGETTI PER UNA REGIONE SEMPRE PIÙ INCLUSIVA E CONNESSA (MILANO, 28.11.2022)',\n",
       "  'video_id': 'EQLRKq8b8Pg'},\n",
       " {'title': 'MATTEO SALVINI A NON STOP NEWS (RTL 102.5, 7.11.2022)',\n",
       "  'video_id': 'hnL4Ysh5oOI'},\n",
       " {'title': 'MATTEO SALVINI A DRITTO E ROVESCIO (RETE 4, 3.11.2022)',\n",
       "  'video_id': 'dEO4XxqxoU8'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_res['videos'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOqQcd9SXdyH"
   },
   "source": [
    "## First Demo - Salvini's Speech at a Coldiretti Conference.\n",
    "\n",
    "Who's better than a populist far-right politician like Salvini to start with the analysis of political speeches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWVfp8E6aFWj",
    "outputId": "10d84a2f-e1bf-4e87-9160-30b07e260e03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://youtu.be/pwhRDzz1UWk',\n",
       " 'https://youtu.be/KYR_IqIP2YI',\n",
       " 'https://youtu.be/EQLRKq8b8Pg',\n",
       " 'https://youtu.be/hnL4Ysh5oOI',\n",
       " 'https://youtu.be/dEO4XxqxoU8']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids = [video['video_id'] for video in parsed_res['videos']]\n",
    "video_urls = ['https://youtu.be/' + video_id for video_id in video_ids]\n",
    "video_urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpxhQWfOa1Nm"
   },
   "source": [
    "### Download & Transcription\n",
    "Download the video from YouTube and transcribe it using whisper. We are going to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U8l7_FVzd_kA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDIRbhnmaoXE",
    "outputId": "a6fba1c5-091c-4b6a-e130-9c24777a31e8"
   },
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_transcribe(url: str):\n",
    "    yt = YouTube(url=url)\n",
    "    title = yt.title\n",
    "    path = yt.streams.filter(only_audio=True)[0].download(filename='audio.mp4')\n",
    "    transcription = whisper_model.transcribe(path, )\n",
    "    return transcription['text'], transcription['language'], title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "rOGmGTTuaylA",
    "outputId": "45eae220-6427-408a-85c1-ac92681d1aca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                        | 0/50 [00:00<?, ?it/s]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "  2%|██▊                                                                                                                                        | 1/50 [17:29<14:16:55, 1049.29s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "  4%|█████▌                                                                                                                                      | 2/50 [30:06<11:42:10, 877.72s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "  6%|████████▎                                                                                                                                  | 3/50 [55:16<15:13:24, 1166.05s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "  8%|██████████▉                                                                                                                              | 4/50 [1:16:38<15:29:17, 1212.13s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 10%|█████████████▋                                                                                                                           | 5/50 [1:37:54<15:26:17, 1235.07s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 12%|████████████████▍                                                                                                                        | 6/50 [2:11:13<18:16:09, 1494.75s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 14%|███████████████████▏                                                                                                                     | 7/50 [2:38:09<18:19:37, 1534.35s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 16%|█████████████████████▉                                                                                                                   | 8/50 [3:01:09<17:19:40, 1485.26s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 18%|████████████████████████▋                                                                                                                | 9/50 [3:41:54<20:19:59, 1785.35s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 20%|███████████████████████████▏                                                                                                            | 10/50 [4:03:42<18:12:01, 1638.04s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 22%|█████████████████████████████▉                                                                                                          | 11/50 [4:19:15<15:24:18, 1422.01s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 24%|████████████████████████████████▋                                                                                                       | 12/50 [5:16:28<21:28:12, 2034.02s/it]/Users/andrea.grillandi/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      " 24%|████████████████████████████████▍                                                                                                      | 12/50 [20:21:29<64:28:04, 6107.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m tqdm(video_urls):\n\u001b[0;32m----> 5\u001b[0m   res \u001b[38;5;241m=\u001b[39m \u001b[43mdo_transcribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m   text, language, title \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mdo_transcribe\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m title \u001b[38;5;241m=\u001b[39m yt\u001b[38;5;241m.\u001b[39mtitle\n\u001b[1;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m yt\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mfilter(only_audio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdownload(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transcription[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], transcription[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m], title\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:181\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, **decode_options)\u001b[0m\n\u001b[1;32m    178\u001b[0m segment_duration \u001b[38;5;241m=\u001b[39m segment\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE\n\u001b[1;32m    180\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[0;32m--> 181\u001b[0m result: DecodingResult \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/transcribe.py:117\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    114\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m options \u001b[38;5;241m=\u001b[39m DecodingOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, temperature\u001b[38;5;241m=\u001b[39mt)\n\u001b[0;32m--> 117\u001b[0m decode_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m needs_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decode_result\u001b[38;5;241m.\u001b[39mcompression_ratio \u001b[38;5;241m>\u001b[39m compression_ratio_threshold:\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/decoding.py:705\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(model, mel, options)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single:\n\u001b[1;32m    703\u001b[0m     mel \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single:\n\u001b[1;32m    708\u001b[0m     result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/decoding.py:637\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    634\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(audio_features\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m tokens, sum_logprobs, no_speech_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[1;32m    640\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m audio_features[:: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group]\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/decoding.py:592\u001b[0m, in \u001b[0;36mDecodingTask._main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_len):\n\u001b[0;32m--> 592\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mno_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[1;32m    595\u001b[0m             probs_at_sot \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_index]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/decoding.py:145\u001b[0m, in \u001b[0;36mPyTorchInference.logits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_token_length:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/model.py:189\u001b[0m, in \u001b[0;36mTextDecoder.forward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    186\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(xa\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 189\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m    192\u001b[0m logits \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/model.py:126\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_ln(x), mask\u001b[38;5;241m=\u001b[39mmask, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m--> 126\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln(x))\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/model.py:85\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m     82\u001b[0m     k \u001b[38;5;241m=\u001b[39m kv_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey]\n\u001b[1;32m     83\u001b[0m     v \u001b[38;5;241m=\u001b[39m kv_cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue]\n\u001b[0;32m---> 85\u001b[0m wv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(wv)\n",
      "File \u001b[0;32m~/Desktop/personal_git_repos/politopic/env/lib/python3.10/site-packages/whisper/model.py:93\u001b[0m, in \u001b[0;36mMultiHeadAttention.qkv_attention\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     91\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mq\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m*\u001b[39m scale\n\u001b[1;32m     92\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mk\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m---> 93\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     95\u001b[0m qk \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "texts = []\n",
    "\n",
    "for url in tqdm(video_urls):\n",
    "  res = do_transcribe(url)\n",
    "  text, language, title = res\n",
    "  if language == 'it':\n",
    "    titles.append(title)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74tWarlojqZY"
   },
   "source": [
    "### Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zbvY9DgkoFk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4R4sxA3iWpf"
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9UeaIMUjwXJ"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "video_titles = []\n",
    "\n",
    "for title, text in zip(titles, texts):\n",
    "  sents = sent_tokenize(text)\n",
    "  docs.extend(sents)\n",
    "  video_titles.extend([title]*len(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-M1KJ4OoksB"
   },
   "source": [
    "Let's save the texts on google drive just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnrpDST7ojji"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Title\": video_titles, \"Text\": docs})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5lTnlsyKj7F"
   },
   "outputs": [],
   "source": [
    "df.groupby('Title').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRXk7zFIpG9b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "drive_path = '/content/drive/MyDrive/Data/political_speeches/politopic_salvini.tsv'\n",
    "if os.path.exists(drive_path):\n",
    "    df.to_csv(drive_path, sep='\\t', index=False)\n",
    "else:\n",
    "    df.to_csv('politopic_salvini.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyRZC6QwfHPO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77mi-Z1-fKkX"
   },
   "outputs": [],
   "source": [
    "drive_path = '/content/drive/MyDrive/Data/political_speeches/politopic_salvini.tsv'\n",
    "if os.path.exists(drive_path):\n",
    "    df = pd.read_csv(drive_path, sep='\\t')\n",
    "else:\n",
    "    df = pd.read_csv('politopic_salvini.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHmec9npKwRN"
   },
   "source": [
    "### Preprocessing\n",
    "Let's normalize the numbers and lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWI111qsK5EL"
   },
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvU3BMaEKv1M"
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG_2KpjJK-06"
   },
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68NvStFNK4NW"
   },
   "outputs": [],
   "source": [
    "def lemmatize(text: str):\n",
    "  doc = nlp(text)\n",
    "  lemmas = []\n",
    "\n",
    "  for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "      if word.upos != \"PUNCT\":\n",
    "        lemmas.append(word.lemma)\n",
    "  return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRZB7687Lbmv"
   },
   "outputs": [],
   "source": [
    "def normalize(text: str):\n",
    "  text = re.sub(\"[0-9]+\", \"NUM\", text)\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqzH8iidrOhd"
   },
   "outputs": [],
   "source": [
    "df.Text = df.Text.apply(lambda text: normalize(text))\n",
    "df.lemmas = df.Text.apply(lambda text: lemmatize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "drive_path = '/content/drive/MyDrive/Data/political_speeches/politopic_salvini.tsv'\n",
    "if os.path.exists(drive_path):\n",
    "    df.to_csv(drive_path, sep='\\t', index=False)\n",
    "else:\n",
    "    df.to_csv('politopic_salvini.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyk-9u0al224"
   },
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXDxvDLQgBN7"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gk2EXKBggE-u"
   },
   "outputs": [],
   "source": [
    "it_stopwords = stopwords.words('italian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RB9IMl1gQs4"
   },
   "outputs": [],
   "source": [
    "it_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhsnlYoikuXD"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26tjypsKekLK"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(\n",
    "    max_df=0.4,\n",
    "    min_df=2,\n",
    "    max_features=1000,\n",
    "    stop_words=it_stopwords\n",
    ")\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyf59wG4gWO0"
   },
   "outputs": [],
   "source": [
    "count = cv.fit_transform(df.lemmas)\n",
    "lda.fit(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZPyypYqjQFo"
   },
   "source": [
    "count.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTL6gKyuhvxN",
    "outputId": "dd4e71b0-d902-41d1-ae91-dc78b13648ea"
   },
   "outputs": [],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eac7PONk8d0"
   },
   "outputs": [],
   "source": [
    "def get_feature_names_per_topic(lda_components, count_vectorizer, topn=5):\n",
    "  topics = []\n",
    "\n",
    "  for i in range(len(lda_components)):\n",
    "    topic = lda_components[i]\n",
    "    top_words_idx = topic.argsort()[-topn:]\n",
    "    topic_names = [count_vectorizer.get_feature_names_out()[id] for id in top_words_idx]\n",
    "    topics.append('_'.join(topic_names))\n",
    "  \n",
    "  return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyRz80x2nB7i"
   },
   "outputs": [],
   "source": [
    "topics = get_feature_names_per_topic(lda.components_, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OP5BwO7onJBW",
    "outputId": "1d44aef2-ae85-41a8-e420-01d3143abca2"
   },
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sqy0VHg1nL9J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNO9MKbnQdRObDrqvqq7ESE",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
